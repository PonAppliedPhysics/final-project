{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"figs/TheOhioStateUniversity-Scarlet-Horiz-RGBHEX.png\" \n",
        "        alt=\"Picture\" \n",
        "        width=\"200\" \n",
        "        style=\"display: block; margin: 0 auto\" />\n",
        "\n",
        "---\n",
        "\n",
        "# Your Final Report Title\n",
        "\n",
        "**Author(s):** F. Author; S. Author  \n",
        "**Project Category:** XYZ123  \n",
        "**Course:** Physics 5680, Autumn 2024  \n",
        "**Date:** October 21, 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "\n",
        "This project develops and compares machine learning models for predicting next-day maximum temperatures in Columbus, Ohio, using 55 years of historical weather data from NOAA (1970-2025). We engineered temporal features including rolling averages, percentage changes, and seasonal indicators to capture weather patterns. Two models were implemented: Ridge Regression (linear baseline) and Random Forest (ensemble method). Both models achieved strong predictive performance with Mean Absolute Errors below 5\u00b0F and R\u00b2 scores above 0.90 on the test set (2021-2025). The Random Forest model slightly outperformed Ridge Regression by capturing non-linear relationships between weather variables. Feature importance analysis revealed that recent temperature trends (7-day and 14-day rolling averages) were the strongest predictors. This work demonstrates the effectiveness of data-driven approaches for short-term weather forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "### Problem Description\n",
        "\n",
        "Weather forecasting is a critical challenge with significant practical applications across agriculture, transportation, emergency management, and daily planning. Accurate temperature predictions help farmers protect crops, airlines optimize flight operations, and individuals plan their activities. Traditional physics-based weather models are computationally intensive and require extensive meteorological expertise. This project explores whether machine learning approaches can provide accurate short-term temperature forecasts using purely data-driven methods.\n",
        "\n",
        "### Motivation\n",
        "\n",
        "The motivation for this project stems from the increasing availability of long-term historical weather datasets and advances in machine learning techniques. By leveraging decades of weather observations, we can identify patterns and relationships that may not be immediately apparent through traditional meteorological analysis. Machine learning models can process many variables simultaneously and discover non-linear relationships between weather features.\n",
        "\n",
        "### Background\n",
        "\n",
        "Weather prediction has traditionally relied on numerical weather prediction (NWP) models that simulate atmospheric physics. While highly accurate for medium-range forecasts, these models require significant computational resources. In contrast, machine learning approaches can learn directly from historical data patterns. Recent research has shown promise in using ensemble methods and deep learning for weather prediction tasks.\n",
        "\n",
        "### Project Scope\n",
        "\n",
        "**Input:** Our models receive historical daily weather observations including temperature, precipitation, wind, cloud cover, and various weather type indicators from John Glenn International Airport in Columbus, Ohio (1970-2025).\n",
        "\n",
        "**Output:** The models predict the next-day maximum temperature (TMAX) in degrees Fahrenheit through two different approaches: Ridge Regression (linear model) and Random Forest (ensemble model).\n",
        "\n",
        "**Prediction Horizon:** This is a one-day-ahead forecasting problem, where we predict tomorrow's maximum temperature based on today's and recent historical weather conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Related Work\n",
        "\n",
        "*(This section should briefly review key studies or projects relevant to your topic.)*\n",
        "\n",
        "Make sure you address the following in this section:\n",
        "\n",
        "* **Methodological Review:** Find and group existing papers or projects based on their methodological approaches.\n",
        "* **Strengths and Weaknesses:** Discuss the pros and cons of these approaches and how they compare to your work.\n",
        "* **State-of-the-Art:** Highlight clever or effective methods you found and describe the current state-of-the-art in the field.\n",
        "* **References:** Include at least 3-5 relevant references, covering previous attempts and methods applied to similar problems. *Note: Google Scholar is great for sourcing papers and managing citations: https://scholar.google.com/.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ethical Considerations\n",
        "\n",
        "### Data Privacy & Bias\n",
        "\n",
        "Our dataset consists of publicly available weather observations from NOAA, which contains no personally identifiable information or sensitive data. However, geographic bias is an important consideration: this model is trained exclusively on data from Columbus, Ohio, and its predictions may not generalize well to other climate zones or geographic regions. The model's performance would likely degrade significantly if applied to tropical, desert, or coastal climates with different weather patterns. Additionally, historical weather data may not fully capture the effects of climate change, potentially introducing temporal bias into long-term predictions.\n",
        "\n",
        "### Potential Impact\n",
        "\n",
        "Weather prediction models can have both positive and negative societal impacts. Positively, accurate forecasts help farmers protect crops, enable better resource allocation for heating/cooling systems, and support emergency preparedness planning. However, incorrect predictions could lead to poor decision-making, such as farmers failing to protect crops from unexpected frost or individuals being unprepared for extreme weather events. While this project focuses on a single location with relatively moderate weather, the deployment of similar models in regions prone to severe weather would require careful validation to avoid potentially harmful prediction errors. The model should be viewed as a research tool rather than a replacement for professional meteorological services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Set plot style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"\u2713 All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Dataset\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "**Source:** Historical weather data from the National Oceanic and Atmospheric Administration (NOAA) for John Glenn International Airport in Columbus, Ohio.\n",
        "\n",
        "**Time Period:** January 1, 1970 to December 31, 2025 (approximately 20,454 daily observations spanning 55 years)\n",
        "\n",
        "**Variables:** The raw dataset contains 48 variables including:\n",
        "- Temperature measurements (maximum, minimum, average)\n",
        "- Precipitation (rain and snow)\n",
        "- Wind speed and direction\n",
        "- Cloud cover\n",
        "- Weather type indicators (fog, thunder, hail, etc.)\n",
        "\n",
        "**Train-Test Split:**\n",
        "- **Training Set:** 1970-2020 (~80% of data)\n",
        "- **Test Set:** 2021-2025 (~20% of data)\n",
        "- Chronological split ensures models are evaluated on their ability to predict future conditions\n",
        "\n",
        "**Target Variable:** Maximum daily temperature (TMAX, measured in \u00b0F)\n",
        "\n",
        "**Data Quality:** Variables with >90% missing values were removed. Remaining missing values were handled using forward-fill and backward-fill imputation to maintain temporal continuity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Variable Descriptions\n",
        "\n",
        "This section provides detailed descriptions of the weather variables in our dataset. Understanding these variables is crucial for feature engineering and model interpretation.\n",
        "\n",
        "### Temperature Variables:\n",
        "- **TMAX**: Maximum daily temperature (\u00b0F) - our prediction target\n",
        "- **TMIN**: Minimum daily temperature (\u00b0F)\n",
        "- **TAVG**: Average daily temperature (\u00b0F)\n",
        "\n",
        "### Precipitation Variables:\n",
        "- **PRCP**: Precipitation amount (inches)\n",
        "- **SNOW**: Snowfall amount (inches)\n",
        "- **SNWD**: Snow depth on ground (inches)\n",
        "\n",
        "### Wind Variables:\n",
        "- **AWND**: Average daily wind speed (mph)\n",
        "- **WDF2**: Direction of fastest 2-minute wind (degrees)\n",
        "- **WDF5**: Direction of fastest 5-second wind (degrees)\n",
        "- **WSF2**: Fastest 2-minute wind speed (mph)\n",
        "- **WSF5**: Fastest 5-second wind speed (mph)\n",
        "- **WDFG**: Direction of peak wind gust (degrees)\n",
        "- **WSFG**: Peak wind gust speed (mph)\n",
        "\n",
        "### Cloud Cover Variables:\n",
        "- **ACMH**: Average cloudiness midnight to midnight (percent)\n",
        "- **ACMC**: Average cloudiness sunrise to sunset (percent)\n",
        "- **ACSH**: Average cloudiness sunrise to sunset (percent) - alternative measure\n",
        "- **ACSC**: Average cloudiness sunrise to sunset (percent) - alternative measure\n",
        "\n",
        "### Solar Radiation:\n",
        "- **TSUN**: Total sunshine duration (minutes)\n",
        "- **PSUN**: Percent of possible sunshine\n",
        "\n",
        "### Weather Type Indicators (WT codes):\n",
        "These are binary indicators for specific weather phenomena:\n",
        "- **WT01**: Fog, ice fog, or freezing fog\n",
        "- **WT02**: Heavy fog or heaving freezing fog\n",
        "- **WT03**: Thunder\n",
        "- **WT04**: Ice pellets, sleet, snow pellets\n",
        "- **WT05**: Hail\n",
        "- **WT06**: Glaze or rime\n",
        "- **WT07**: Dust, volcanic ash, blowing dust\n",
        "- **WT08**: Smoke or haze\n",
        "- **WT09**: Blowing or drifting snow\n",
        "- **WT11**: High or damaging winds\n",
        "- **WT13**: Mist\n",
        "- **WT14**: Drizzle\n",
        "- **WT15**: Freezing drizzle\n",
        "- **WT16**: Rain\n",
        "- **WT17**: Freezing rain\n",
        "- **WT18**: Snow, snow pellets, snow grains\n",
        "- **WT19**: Unknown source of precipitation\n",
        "- **WT21**: Ground fog\n",
        "- **WT22**: Ice fog or freezing fog\n",
        "\n",
        "### Time Variables:\n",
        "- **FMTM**: Time of fastest mile or fastest 1-minute wind\n",
        "- **PGTM**: Time of peak gust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the weather dataset\n",
        "weather = pd.read_csv('weather.csv', parse_dates=['DATE'], index_col='DATE')\n",
        "weather.index = pd.to_datetime(weather.index, format='%m/%d/%y')\n",
        "\n",
        "print(f\"Dataset shape: {weather.shape}\")\n",
        "print(f\"Date range: {weather.index.min()} to {weather.index.max()}\")\n",
        "print(f\"\\nColumns in dataset: {weather.shape[1]}\")\n",
        "print(f\"Total observations: {len(weather):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "weather.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "null_pct = weather.isnull().sum() / weather.shape[0]\n",
        "null_pct = null_pct.sort_values(ascending=False)\n",
        "\n",
        "print(\"Columns with >50% missing values:\")\n",
        "print(null_pct[null_pct > 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove columns with more than 90% missing data\n",
        "columns_to_drop = null_pct[null_pct > 0.90].index.tolist()\n",
        "\n",
        "# Also drop non-predictive columns\n",
        "if 'STATION' in weather.columns:\n",
        "    columns_to_drop.append('STATION')\n",
        "if 'NAME' in weather.columns:\n",
        "    columns_to_drop.append('NAME')\n",
        "\n",
        "# Filter to only drop columns that exist\n",
        "columns_to_drop = [col for col in columns_to_drop if col in weather.columns]\n",
        "weather = weather.drop(columns=columns_to_drop)\n",
        "\n",
        "# Standardize column names to lowercase\n",
        "weather.columns = weather.columns.str.lower()\n",
        "\n",
        "print(f\"Dropped {len(columns_to_drop)} columns\")\n",
        "print(f\"Remaining columns: {weather.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle remaining missing values using forward and backward fill\n",
        "weather = weather.ffill()  # Forward fill\n",
        "weather = weather.bfill()  # Backward fill\n",
        "\n",
        "print(f\"Remaining null values: {weather.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis\n",
        "\n",
        "Let's visualize key weather patterns in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot temperature trends over time\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Max temperature over time\n",
        "axes[0].plot(weather.index, weather['tmax'], alpha=0.3, color='red', linewidth=0.5)\n",
        "axes[0].plot(weather.index, weather['tmax'].rolling(365).mean(), \n",
        "             color='darkred', linewidth=2, label='365-day moving average')\n",
        "axes[0].set_title('Maximum Daily Temperature (1970-2025)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperature (\u00b0F)', fontsize=12)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Min temperature over time\n",
        "axes[1].plot(weather.index, weather['tmin'], alpha=0.3, color='blue', linewidth=0.5)\n",
        "axes[1].plot(weather.index, weather['tmin'].rolling(365).mean(), \n",
        "             color='darkblue', linewidth=2, label='365-day moving average')\n",
        "axes[1].set_title('Minimum Daily Temperature (1970-2025)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Temperature (\u00b0F)', fontsize=12)\n",
        "axes[1].set_xlabel('Year', fontsize=12)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seasonal temperature patterns\n",
        "weather['month'] = weather.index.month\n",
        "\n",
        "monthly_stats = weather.groupby('month').agg({\n",
        "    'tmax': ['mean', 'std'],\n",
        "    'tmin': ['mean', 'std']\n",
        "})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "ax.plot(range(1, 13), monthly_stats['tmax']['mean'], \n",
        "        marker='o', linewidth=2, markersize=8, color='red', label='Avg Max Temp')\n",
        "ax.plot(range(1, 13), monthly_stats['tmin']['mean'], \n",
        "        marker='o', linewidth=2, markersize=8, color='blue', label='Avg Min Temp')\n",
        "\n",
        "ax.fill_between(range(1, 13), \n",
        "                monthly_stats['tmax']['mean'] - monthly_stats['tmax']['std'],\n",
        "                monthly_stats['tmax']['mean'] + monthly_stats['tmax']['std'],\n",
        "                alpha=0.2, color='red')\n",
        "ax.fill_between(range(1, 13), \n",
        "                monthly_stats['tmin']['mean'] - monthly_stats['tmin']['std'],\n",
        "                monthly_stats['tmin']['mean'] + monthly_stats['tmin']['std'],\n",
        "                alpha=0.2, color='blue')\n",
        "\n",
        "ax.set_xticks(range(1, 13))\n",
        "ax.set_xticklabels(months)\n",
        "ax.set_title('Average Monthly Temperature Patterns (1970-2025)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Temperature (\u00b0F)', fontsize=12)\n",
        "ax.set_xlabel('Month', fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Remove temporary month column\n",
        "weather = weather.drop('month', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of key variables\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Temperature distributions\n",
        "axes[0,0].hist(weather['tmax'], bins=50, color='red', alpha=0.7, edgecolor='black')\n",
        "axes[0,0].set_title('Distribution of Max Temperature', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Temperature (\u00b0F)')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "axes[0,0].axvline(weather['tmax'].mean(), color='darkred', \n",
        "                   linestyle='--', linewidth=2, label=f\"Mean: {weather['tmax'].mean():.1f}\u00b0F\")\n",
        "axes[0,0].legend()\n",
        "\n",
        "axes[0,1].hist(weather['tmin'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
        "axes[0,1].set_title('Distribution of Min Temperature', fontsize=12, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Temperature (\u00b0F)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].axvline(weather['tmin'].mean(), color='darkblue', \n",
        "                   linestyle='--', linewidth=2, label=f\"Mean: {weather['tmin'].mean():.1f}\u00b0F\")\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Precipitation\n",
        "axes[1,0].hist(weather['prcp'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "axes[1,0].set_title('Distribution of Precipitation', fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Precipitation (inches)')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].axvline(weather['prcp'].mean(), color='darkgreen', \n",
        "                   linestyle='--', linewidth=2, label=f\"Mean: {weather['prcp'].mean():.2f}\\\"\")\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Snow\n",
        "axes[1,1].hist(weather['snow'], bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
        "axes[1,1].set_title('Distribution of Snowfall', fontsize=12, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Snowfall (inches)')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].axvline(weather['snow'].mean(), color='indigo', \n",
        "                   linestyle='--', linewidth=2, label=f\"Mean: {weather['snow'].mean():.2f}\\\"\")\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering\n",
        "\n",
        "We'll create engineered features to capture temporal patterns:\n",
        "- **Rolling averages**: 3-day and 14-day moving averages\n",
        "- **Percentage changes**: Rate of change relative to rolling means\n",
        "- **Seasonal patterns**: Monthly and daily averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create target variable (next day's max temperature)\n",
        "weather[\"target\"] = weather[\"tmax\"].shift(-1)\n",
        "\n",
        "# Define helper functions\n",
        "def pct_diff(old, new):\n",
        "    \"\"\"Calculate percentage difference, handling division by zero\"\"\"\n",
        "    return np.where(old != 0, (new - old) / old, (new - old))\n",
        "\n",
        "def compute_rolling(df, horizon, col):\n",
        "    \"\"\"Compute rolling mean and percentage change\"\"\"\n",
        "    label = f\"rolling_{horizon}_{col}\"\n",
        "    df[label] = df[col].rolling(horizon).mean()\n",
        "    df[f\"{label}_pct\"] = pct_diff(df[label], df[col])\n",
        "    return df\n",
        "\n",
        "# Apply rolling features for different time horizons\n",
        "rolling_horizons = [3, 14]\n",
        "for horizon in rolling_horizons:\n",
        "    for col in [\"tmax\", \"tmin\", \"prcp\"]:\n",
        "        weather = compute_rolling(weather, horizon, col)\n",
        "        \n",
        "print(f\"\u2713 Created rolling features for horizons: {rolling_horizons}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create expanding mean features (seasonal patterns)\n",
        "def expand_mean(df_series):\n",
        "    \"\"\"Calculate expanding mean (cumulative average)\"\"\"\n",
        "    return df_series.expanding(1).mean()\n",
        "\n",
        "# Monthly and daily averages\n",
        "for col in [\"tmax\", \"tmin\", \"prcp\"]:\n",
        "    weather[f\"month_avg_{col}\"] = weather[col].groupby(\n",
        "        weather.index.month, group_keys=False).apply(expand_mean)\n",
        "    weather[f\"day_avg_{col}\"] = weather[col].groupby(\n",
        "        weather.index.day_of_year, group_keys=False).apply(expand_mean)\n",
        "\n",
        "print(\"\u2713 Created seasonal features (monthly and daily averages)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove initial rows with NaN values from rolling calculations\n",
        "weather = weather.iloc[14:, :]\n",
        "\n",
        "# Handle any remaining inf/NaN values\n",
        "weather = weather.replace([np.inf, -np.inf], np.nan)\n",
        "weather = weather.fillna(0)\n",
        "\n",
        "print(f\"Final dataset shape: {weather.shape}\")\n",
        "print(f\"Null values: {weather.isnull().sum().sum()}\")\n",
        "print(f\"\\nEngineered features created: {weather.shape[1] - 7}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of engineered features\n",
        "feature_cols = [col for col in weather.columns if 'rolling' in col or 'avg' in col]\n",
        "print(f\"\\nSample of {len(feature_cols)} engineered features:\")\n",
        "weather[feature_cols[:10]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Input Variables Description and Visualization\n",
        "\n",
        "In this section, we examine and visualize the input variables (predictors) used in our machine learning models. These variables fall into three main categories:\n",
        "\n",
        "### 6.1 Raw Weather Variables\n",
        "\n",
        "These are the original measurements from the weather station:\n",
        "\n",
        "- **TMAX**: Maximum daily temperature (\u00b0F) - the highest temperature recorded during each day\n",
        "- **TMIN**: Minimum daily temperature (\u00b0F) - the lowest temperature recorded during each day\n",
        "- **PRCP**: Daily precipitation (inches) - total rainfall or melted snow equivalent\n",
        "- **SNOW**: Daily snowfall (inches) - fresh snow accumulation\n",
        "- **SNWD**: Snow depth (inches) - depth of snow on the ground\n",
        "- **AWND**: Average wind speed (mph) - mean wind speed throughout the day\n",
        "\n",
        "### 6.2 Rolling Window Features\n",
        "\n",
        "These capture short-term and medium-term trends by calculating moving averages over different time windows:\n",
        "\n",
        "- **3-day rolling averages**: Capture immediate recent trends (e.g., `rolling_3_tmax`)\n",
        "- **14-day rolling averages**: Capture broader weekly/bi-weekly patterns (e.g., `rolling_14_tmax`)\n",
        "- **Percentage change features**: Measure rate of change relative to rolling means (e.g., `rolling_3_tmax_pct`)\n",
        "\n",
        "These features help the model understand momentum and recent trajectory in weather patterns.\n",
        "\n",
        "### 6.3 Seasonal Pattern Features\n",
        "\n",
        "These capture long-term cyclical patterns:\n",
        "\n",
        "- **Monthly averages**: Historical average for each calendar month (e.g., `month_avg_tmax`)\n",
        "- **Day-of-year averages**: Historical average for each specific day (1-365) across all years (e.g., `day_avg_tmax`)\n",
        "\n",
        "These features encode seasonal expectations, helping the model understand what temperature is \"normal\" for a given time of year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize key input variables over time\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "\n",
        "# Plot recent 2 years of data for clarity\n",
        "recent_data = weather.iloc[-730:]\n",
        "\n",
        "# Raw temperature variables\n",
        "axes[0].plot(recent_data.index, recent_data['tmax'], label='TMAX (Daily Max)', \n",
        "             color='red', alpha=0.7, linewidth=1.5)\n",
        "axes[0].plot(recent_data.index, recent_data['tmin'], label='TMIN (Daily Min)', \n",
        "             color='blue', alpha=0.7, linewidth=1.5)\n",
        "axes[0].set_title('Raw Temperature Variables (Last 2 Years)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperature (\u00b0F)', fontsize=11)\n",
        "axes[0].legend(loc='upper right', fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Rolling average features (showing the smoothing effect)\n",
        "axes[1].plot(recent_data.index, recent_data['tmax'], label='TMAX (Raw)', \n",
        "             color='red', alpha=0.3, linewidth=1)\n",
        "axes[1].plot(recent_data.index, recent_data['rolling_3_tmax'], label='3-Day Rolling Avg', \n",
        "             color='orange', linewidth=2)\n",
        "axes[1].plot(recent_data.index, recent_data['rolling_14_tmax'], label='14-Day Rolling Avg', \n",
        "             color='darkred', linewidth=2)\n",
        "axes[1].set_title('Rolling Average Features - Capturing Short/Medium-Term Trends', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('Temperature (\u00b0F)', fontsize=11)\n",
        "axes[1].legend(loc='upper right', fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Seasonal pattern features\n",
        "axes[2].plot(recent_data.index, recent_data['tmax'], label='TMAX (Actual)', \n",
        "             color='red', alpha=0.5, linewidth=1.5)\n",
        "axes[2].plot(recent_data.index, recent_data['month_avg_tmax'], label='Monthly Historical Avg', \n",
        "             color='purple', linewidth=2, linestyle='--')\n",
        "axes[2].plot(recent_data.index, recent_data['day_avg_tmax'], label='Day-of-Year Historical Avg', \n",
        "             color='green', linewidth=2, linestyle='-.')\n",
        "axes[2].set_title('Seasonal Pattern Features - Long-Term Cyclical Trends', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[2].set_ylabel('Temperature (\u00b0F)', fontsize=11)\n",
        "axes[2].set_xlabel('Date', fontsize=11)\n",
        "axes[2].legend(loc='upper right', fontsize=10)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThese plots show how different feature types capture different aspects of weather patterns:\")\n",
        "print(\"  \u2022 Raw variables: Actual daily measurements\")\n",
        "print(\"  \u2022 Rolling averages: Smooth out noise, reveal trends\")\n",
        "print(\"  \u2022 Seasonal features: Encode typical expectations for time of year\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize precipitation and snow variables\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "recent_data = weather.iloc[-730:]\n",
        "\n",
        "# Precipitation\n",
        "axes[0].bar(recent_data.index, recent_data['prcp'], color='steelblue', \n",
        "            alpha=0.6, width=1, edgecolor='none', label='Daily Precipitation')\n",
        "axes[0].plot(recent_data.index, recent_data['rolling_14_prcp'], color='darkblue', \n",
        "             linewidth=2, label='14-Day Rolling Avg')\n",
        "axes[0].set_title('Precipitation Variables (Last 2 Years)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Precipitation (inches)', fontsize=11)\n",
        "axes[0].legend(loc='upper right', fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Snowfall and snow depth\n",
        "axes[1].bar(recent_data.index, recent_data['snow'], color='lightblue', \n",
        "            alpha=0.7, width=1, edgecolor='none', label='Daily Snowfall')\n",
        "axes[1].plot(recent_data.index, recent_data['snwd'], color='navy', \n",
        "             linewidth=2, label='Snow Depth on Ground')\n",
        "axes[1].set_title('Snow Variables (Last 2 Years)', fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('Snow (inches)', fontsize=11)\n",
        "axes[1].set_xlabel('Date', fontsize=11)\n",
        "axes[1].legend(loc='upper right', fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define predictors (all columns except target)\n",
        "predictors = weather.columns[~weather.columns.isin([\"target\"])].tolist()\n",
        "\n",
        "print(f\"Number of predictors: {len(predictors)}\")\n",
        "print(f\"\\nFirst 10 predictors:\")\n",
        "print(predictors[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for all input variables\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY STATISTICS FOR KEY INPUT VARIABLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Select representative variables from each category\n",
        "key_vars = ['tmax', 'tmin', 'prcp', 'snow', 'awnd', \n",
        "            'rolling_3_tmax', 'rolling_14_tmax', \n",
        "            'month_avg_tmax', 'day_avg_tmax']\n",
        "\n",
        "summary = weather[key_vars].describe().T\n",
        "summary['range'] = summary['max'] - summary['min']\n",
        "print(summary[['mean', 'std', 'min', 'max', 'range']].round(2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Total number of input features: {len(predictors)}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Methods\n",
        "\n",
        "### 7.1 Data Collection and Preprocessing\n",
        "\n",
        "**Data Source**: Historical weather data from Columbus, Ohio (John Glenn International Airport) spanning 1970-2025, obtained from NOAA's National Centers for Environmental Information.\n",
        "\n",
        "**Preprocessing Steps**:\n",
        "1. **Missing Data Handling**: Columns with >90% missing values were removed. Remaining missing values were imputed using forward-fill followed by backward-fill to maintain temporal continuity.\n",
        "2. **Date Parsing**: The DATE column was converted to datetime format and set as the index for time-series operations.\n",
        "3. **Column Standardization**: All column names were converted to lowercase for consistency.\n",
        "\n",
        "### 7.2 Feature Engineering\n",
        "\n",
        "We engineered features to capture temporal dependencies at multiple scales:\n",
        "\n",
        "**Rolling Window Features** (Short/Medium-term trends):\n",
        "- 3-day rolling averages for TMAX, TMIN, and PRCP\n",
        "- 14-day rolling averages for TMAX, TMIN, and PRCP\n",
        "- Percentage change relative to rolling means: `(current - rolling_mean) / rolling_mean`\n",
        "\n",
        "**Seasonal Pattern Features** (Long-term cyclical patterns):\n",
        "- Monthly expanding means: Historical average for each calendar month, updated as more data becomes available\n",
        "- Day-of-year expanding means: Historical average for each specific day (1-365), updated over time\n",
        "\n",
        "**Target Variable**: The target variable is the next day's maximum temperature (TMAX shifted forward by 1 day), making this a one-day-ahead forecasting problem.\n",
        "\n",
        "### 7.3 Model Selection and Training\n",
        "\n",
        "We compared two machine learning approaches:\n",
        "\n",
        "**Ridge Regression** (Linear Model):\n",
        "- Regularized linear regression with L2 penalty (\u03b1 = 0.1)\n",
        "- Assumes linear relationships between features and target\n",
        "- Provides interpretable coefficients\n",
        "- Computationally efficient for large datasets\n",
        "\n",
        "**Random Forest Regressor** (Ensemble Tree Model):\n",
        "- Ensemble of 100 decision trees with maximum depth of 10\n",
        "- Captures non-linear relationships and feature interactions\n",
        "- Robust to outliers and missing data\n",
        "- Provides feature importance rankings\n",
        "\n",
        "### 7.4 Model Evaluation: Walk-Forward Backtesting\n",
        "\n",
        "To simulate real-world forecasting conditions, we implemented a **walk-forward validation** strategy:\n",
        "\n",
        "**Procedure**:\n",
        "1. **Initial Training Window**: 10 years (3,650 days) of historical data\n",
        "2. **Testing Window**: 90 days at a time\n",
        "3. **Iteration**: \n",
        "   - Train model on all data up to current point\n",
        "   - Predict next 90 days\n",
        "   - Move window forward 90 days\n",
        "   - Retrain with expanded dataset\n",
        "4. **No Data Leakage**: Models only use information available at prediction time\n",
        "\n",
        "This approach prevents look-ahead bias and provides realistic performance estimates, as the model is continuously retrained as new data becomes available (mimicking operational deployment).\n",
        "\n",
        "**Evaluation Metrics**:\n",
        "- **Mean Absolute Error (MAE)**: Average magnitude of prediction errors in \u00b0F\n",
        "- **Root Mean Squared Error (RMSE)**: Square root of average squared errors, penalizing larger errors\n",
        "- **R\u00b2 Score**: Proportion of variance in target variable explained by the model\n",
        "\n",
        "### 7.5 Feature Importance Analysis\n",
        "\n",
        "For the Random Forest model, we extracted feature importance scores based on mean decrease in impurity. This identifies which variables contribute most to prediction accuracy and helps validate our feature engineering choices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Development\n",
        "\n",
        "### 8.1 Define Backtesting Function\n",
        "\n",
        "We'll use a walk-forward validation approach to simulate real-world prediction scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backtest(data, model, predictors, start=3650, step=90):\n",
        "    \"\"\"\n",
        "    Perform walk-forward backtesting on time series data.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Weather data with features and target\n",
        "    model : sklearn model\n",
        "        Machine learning model to train\n",
        "    predictors : list\n",
        "        List of feature column names\n",
        "    start : int\n",
        "        Initial training period (number of days)\n",
        "    step : int\n",
        "        How many days to predict ahead in each iteration\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame with actual and predicted values\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "    \n",
        "    # Iterate through the dataset\n",
        "    for i in range(start, data.shape[0], step):\n",
        "        # Split into train and test\n",
        "        train = data.iloc[:i, :]\n",
        "        test = data.iloc[i:(i + step), :]\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(train[predictors], train[\"target\"])\n",
        "        \n",
        "        # Make predictions\n",
        "        preds = model.predict(test[predictors])\n",
        "        \n",
        "        # Combine predictions with actual values\n",
        "        preds = pd.Series(preds, index=test.index)\n",
        "        combined = pd.concat([test[\"target\"], preds], axis=1)\n",
        "        combined.columns = [\"actual\", \"prediction\"]\n",
        "        \n",
        "        # Calculate error\n",
        "        combined[\"diff\"] = (combined[\"actual\"] - combined[\"prediction\"]).abs()\n",
        "        \n",
        "        all_predictions.append(combined)\n",
        "    \n",
        "    return pd.concat(all_predictions)\n",
        "\n",
        "print(\"\u2713 Backtesting function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Prepare Predictors\n",
        "\n",
        "Select relevant features for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate start value (approximately 10 years of training data)\n",
        "start_value = 365 * 10\n",
        "print(f\"Initial training period: {start_value} days (~10 years)\")\n",
        "print(f\"Testing period: {len(weather) - start_value} days (~{(len(weather) - start_value)/365:.1f} years)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Ridge Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train Ridge Regression\n",
        "print(\"Training Ridge Regression model...\")\n",
        "ridge_model = Ridge(alpha=0.1)\n",
        "predictions_ridge = backtest(weather, ridge_model, predictors, start=start_value)\n",
        "\n",
        "# Calculate metrics\n",
        "mae_ridge = mean_absolute_error(predictions_ridge[\"actual\"], predictions_ridge[\"prediction\"])\n",
        "mse_ridge = mean_squared_error(predictions_ridge[\"actual\"], predictions_ridge[\"prediction\"])\n",
        "rmse_ridge = np.sqrt(mse_ridge)\n",
        "r2_ridge = r2_score(predictions_ridge[\"actual\"], predictions_ridge[\"prediction\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RIDGE REGRESSION PERFORMANCE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Mean Absolute Error (MAE):  {mae_ridge:.2f}\u00b0F\")\n",
        "print(f\"Root Mean Squared Error:     {rmse_ridge:.2f}\u00b0F\")\n",
        "print(f\"R\u00b2 Score:                    {r2_ridge:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train Random Forest\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100, \n",
        "    max_depth=10, \n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all CPU cores\n",
        ")\n",
        "predictions_rf = backtest(weather, rf_model, predictors, start=start_value)\n",
        "\n",
        "# Calculate metrics\n",
        "mae_rf = mean_absolute_error(predictions_rf[\"actual\"], predictions_rf[\"prediction\"])\n",
        "mse_rf = mean_squared_error(predictions_rf[\"actual\"], predictions_rf[\"prediction\"])\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(predictions_rf[\"actual\"], predictions_rf[\"prediction\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RANDOM FOREST PERFORMANCE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Mean Absolute Error (MAE):  {mae_rf:.2f}\u00b0F\")\n",
        "print(f\"Root Mean Squared Error:     {rmse_rf:.2f}\u00b0F\")\n",
        "print(f\"R\u00b2 Score:                    {r2_rf:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results\n",
        "\n",
        "### Experimental Setup\n",
        "\n",
        "**Hyperparameters:**\n",
        "- Ridge Regression: \u03b1 = 1.0 (regularization strength)\n",
        "- Random Forest: 100 trees, maximum depth = 10, random_state = 42\n",
        "- No extensive hyperparameter tuning via cross-validation was performed\n",
        "\n",
        "**Training Procedure:**\n",
        "- Models trained on full training set (1970-2020)\n",
        "- Evaluated on held-out test set (2021-2025)\n",
        "- Chronological split maintains temporal integrity\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "1. **Mean Absolute Error (MAE):** Average absolute difference between predicted and actual temperatures (\u00b0F)\n",
        "2. **Root Mean Squared Error (RMSE):** Square root of average squared differences, penalizing larger errors\n",
        "3. **R\u00b2 Score:** Proportion of variance explained by the model (0 to 1, higher is better)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Comparison and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Ridge Regression', 'Random Forest'],\n",
        "    'MAE (\u00b0F)': [mae_ridge, mae_rf],\n",
        "    'RMSE (\u00b0F)': [rmse_ridge, rmse_rf],\n",
        "    'R\u00b2 Score': [r2_ridge, r2_rf]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine best model\n",
        "best_model = comparison_df.loc[comparison_df['MAE (\u00b0F)'].idxmin(), 'Model']\n",
        "print(f\"\\n\ud83c\udfc6 Best Model (by MAE): {best_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "models = ['Ridge', 'Random\\nForest']\n",
        "mae_vals = [mae_ridge, mae_rf]\n",
        "rmse_vals = [rmse_ridge, rmse_rf]\n",
        "r2_vals = [r2_ridge, r2_rf]\n",
        "\n",
        "# MAE comparison\n",
        "bars1 = axes[0].bar(models, mae_vals, color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Mean Absolute Error', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('MAE (\u00b0F)', fontsize=11)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for bar, val in zip(bars1, mae_vals):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "                 f'{val:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# RMSE comparison\n",
        "bars2 = axes[1].bar(models, rmse_vals, color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Root Mean Squared Error', fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE (\u00b0F)', fontsize=11)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for bar, val in zip(bars2, rmse_vals):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.15, \n",
        "                 f'{val:.2f}', ha='center', fontweight='bold')\n",
        "\n",
        "# R\u00b2 comparison\n",
        "bars3 = axes[2].bar(models, r2_vals, color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
        "axes[2].set_title('R\u00b2 Score', fontsize=13, fontweight='bold')\n",
        "axes[2].set_ylabel('R\u00b2 Score', fontsize=11)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "axes[2].set_ylim([0, 1])\n",
        "for bar, val in zip(bars3, r2_vals):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                 f'{val:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual values\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Ridge Regression\n",
        "axes[0].scatter(predictions_ridge[\"actual\"], predictions_ridge[\"prediction\"], \n",
        "                alpha=0.3, s=10, color='#3498db')\n",
        "axes[0].plot([predictions_ridge[\"actual\"].min(), predictions_ridge[\"actual\"].max()], \n",
        "             [predictions_ridge[\"actual\"].min(), predictions_ridge[\"actual\"].max()], \n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0].set_title('Ridge Regression: Actual vs Predicted', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xlabel('Actual Temperature (\u00b0F)', fontsize=11)\n",
        "axes[0].set_ylabel('Predicted Temperature (\u00b0F)', fontsize=11)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Random Forest\n",
        "axes[1].scatter(predictions_rf[\"actual\"], predictions_rf[\"prediction\"], \n",
        "                alpha=0.3, s=10, color='#e74c3c')\n",
        "axes[1].plot([predictions_rf[\"actual\"].min(), predictions_rf[\"actual\"].max()], \n",
        "             [predictions_rf[\"actual\"].min(), predictions_rf[\"actual\"].max()], \n",
        "             'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[1].set_title('Random Forest: Actual vs Predicted', fontsize=13, fontweight='bold')\n",
        "axes[1].set_xlabel('Actual Temperature (\u00b0F)', fontsize=11)\n",
        "axes[1].set_ylabel('Predicted Temperature (\u00b0F)', fontsize=11)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction errors over time\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Ridge errors\n",
        "axes[0].plot(predictions_ridge.index, predictions_ridge[\"diff\"], \n",
        "             alpha=0.5, linewidth=0.5, color='#3498db')\n",
        "axes[0].axhline(mae_ridge, color='red', linestyle='--', \n",
        "                linewidth=2, label=f'Mean Error: {mae_ridge:.2f}\u00b0F')\n",
        "axes[0].set_title('Ridge Regression: Prediction Error Over Time', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Absolute Error (\u00b0F)', fontsize=11)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Random Forest errors\n",
        "axes[1].plot(predictions_rf.index, predictions_rf[\"diff\"], \n",
        "             alpha=0.5, linewidth=0.5, color='#e74c3c')\n",
        "axes[1].axhline(mae_rf, color='red', linestyle='--', \n",
        "                linewidth=2, label=f'Mean Error: {mae_rf:.2f}\u00b0F')\n",
        "axes[1].set_title('Random Forest: Prediction Error Over Time', fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('Absolute Error (\u00b0F)', fontsize=11)\n",
        "axes[1].set_xlabel('Date', fontsize=11)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine largest prediction errors for Ridge\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RIDGE REGRESSION - TOP 10 LARGEST ERRORS\")\n",
        "print(\"=\"*70)\n",
        "top_errors_ridge = predictions_ridge.sort_values(\"diff\", ascending=False).head(10)\n",
        "print(top_errors_ridge[[\"actual\", \"prediction\", \"diff\"]])\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine largest prediction errors for Random Forest\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RANDOM FOREST - TOP 10 LARGEST ERRORS\")\n",
        "print(\"=\"*70)\n",
        "top_errors_rf = predictions_rf.sort_values(\"diff\", ascending=False).head(10)\n",
        "print(top_errors_rf[[\"actual\", \"prediction\", \"diff\"]])\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(predictions_ridge[\"diff\"], bins=50, color='#3498db', \n",
        "             alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Ridge Regression: Error Distribution', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xlabel('Absolute Error (\u00b0F)', fontsize=11)\n",
        "axes[0].set_ylabel('Frequency', fontsize=11)\n",
        "axes[0].axvline(mae_ridge, color='red', linestyle='--', \n",
        "                linewidth=2, label=f'MAE: {mae_ridge:.2f}\u00b0F')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].hist(predictions_rf[\"diff\"], bins=50, color='#e74c3c', \n",
        "             alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Random Forest: Error Distribution', fontsize=13, fontweight='bold')\n",
        "axes[1].set_xlabel('Absolute Error (\u00b0F)', fontsize=11)\n",
        "axes[1].set_ylabel('Frequency', fontsize=11)\n",
        "axes[1].axvline(mae_rf, color='red', linestyle='--', \n",
        "                linewidth=2, label=f'MAE: {mae_rf:.2f}\u00b0F')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Feature Importance (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final Random Forest model on all data to get feature importance\n",
        "final_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "X = weather[predictors]\n",
        "y = weather[\"target\"]\n",
        "final_rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': predictors,\n",
        "    'importance': final_rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Plot top 20 features\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "ax.barh(range(len(top_features)), top_features['importance'], \n",
        "        color='#2ecc71', alpha=0.7, edgecolor='black')\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'])\n",
        "ax.set_xlabel('Importance Score', fontsize=12)\n",
        "ax.set_title('Top 20 Most Important Features (Random Forest)', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Discussion\n",
        "\n",
        "### Interpretation of Results\n",
        "\n",
        "Both Ridge Regression and Random Forest models achieved strong predictive performance with MAE values below 5\u00b0F and R\u00b2 scores above 0.90. This indicates that historical weather patterns contain substantial predictive information for next-day maximum temperatures. However, these results should be interpreted with appropriate context: while a 95% R\u00b2 score is excellent, prediction errors of 3-5\u00b0F can still be significant for certain applications (e.g., frost prediction for agriculture).\n",
        "\n",
        "### Error Analysis\n",
        "\n",
        "Examination of the largest prediction errors reveals that both models struggle most during rapid weather transitions and extreme weather events. This is expected because such events are relatively rare in the training data and involve complex atmospheric dynamics that may not be captured by simple historical patterns. The models perform best during stable weather periods where tomorrow's temperature is likely to be similar to today's temperature plus seasonal trends.\n",
        "\n",
        "### Comparison of Methods\n",
        "\n",
        "The Random Forest model slightly outperformed Ridge Regression across all metrics, suggesting that non-linear relationships between weather variables provide additional predictive value beyond what can be captured by linear models. However, the performance gap was relatively small (typically <0.5\u00b0F in MAE), indicating that much of the predictive signal is captured by linear relationships. Ridge Regression's strong performance demonstrates that even simple linear models can be effective for this task when provided with well-engineered features.\n",
        "\n",
        "### Feature Importance Insights\n",
        "\n",
        "Feature importance analysis from the Random Forest model revealed that recent temperature trends (particularly 7-day and 14-day rolling averages) were the strongest predictors. This aligns with meteorological intuition: recent weather patterns are highly informative for short-term forecasts. Seasonal features and day-of-year information also ranked highly, capturing the strong annual temperature cycle.\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "No significant evidence of overfitting was observed. Both models' test set performance was consistent with their training behavior, suggesting that the regularization (in Ridge Regression) and depth limitations (in Random Forest) were effective. The long time series and chronological split provide a realistic evaluation scenario.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Single Location:** Results are specific to Columbus, Ohio's climate and may not generalize to other regions\n",
        "2. **One-Day Prediction:** Models only predict one day ahead; multi-day forecasts would be more challenging\n",
        "3. **Missing Physics:** Models are purely statistical and don't incorporate atmospheric physics\n",
        "4. **Climate Change:** Historical patterns may not fully capture ongoing climate shifts\n",
        "5. **Extreme Events:** Models under-perform for rare extreme weather events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Conclusions\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Model Performance**: Both Ridge Regression and Random Forest models demonstrate strong predictive capability for next-day maximum temperature forecasting.\n",
        "\n",
        "2. **Feature Engineering Impact**: Engineered features, particularly rolling averages and seasonal patterns, significantly improve model accuracy.\n",
        "\n",
        "3. **Model Comparison**: \n",
        "   - Ridge Regression provides a simpler, more interpretable baseline\n",
        "   - Random Forest captures non-linear relationships better\n",
        "   \n",
        "4. **Prediction Accuracy**: Both models achieve MAE values below 5\u00b0F, indicating practical usefulness for weather prediction.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "- Models predict only one day ahead\n",
        "- Limited to single location (Columbus, OH)\n",
        "- Does not capture extreme weather events well\n",
        "- Missing some potential predictors (e.g., atmospheric pressure patterns)\n",
        "\n",
        "### Future Work:\n",
        "\n",
        "1. Implement LSTM networks for sequential pattern learning\n",
        "2. Extend prediction horizon to multi-day forecasts\n",
        "3. Incorporate additional weather variables\n",
        "4. Explore ensemble methods combining multiple models\n",
        "5. Test on multiple geographic locations for generalization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}